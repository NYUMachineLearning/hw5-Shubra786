---
title: 'Machine Learning 2019: Tree-Based Methods'
author: "Sonali Narang"
date: "10/28/2019"
output:
  pdf_document: default
  pdf: default
---

## Tree-Based Methods 

Decision tree is a type of supervised learning algorithm that can be used in both regression and classification problems. Tree-based methods works for both categorical and continuous input and output variables.

```{r load relevant libraries, include=FALSE}
library(tidyverse)
library(ISLR)
library(tree)
library(randomForest)
library(MASS)
library(gbm)
library(rpart)
library(rpart.plot)
library(partykit)
library(grid)
library(libcoin)
library(caret)
library(pROC)
```
## The Carseats Dataset 

400 Observations, 11 variables
Response Variable: Sales/High 

```{r The Carseats Dataset}
data("Carseats")
carseats = Carseats
head(carseats)

#convert quantitative variable Sales into a binary response 
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)

head(carseats)
```

## Classification Tree

Input variables (X) can be continuous or categorical.
Response variable (Y) is categorical (usually binary): in this case Sales/High.

```{r Classification Tree}
#set seed to make results reproducible 
set.seed(29)

#split data into train and test subset (250 and 150 respectively)
train = sample(1:nrow(carseats), 250)

#Fit train subset of data to model 
tree.carseats = tree(High~.-Sales, carseats, subset=train)
summary(tree.carseats)

#Visualize tree
plot(tree.carseats)
text(tree.carseats, pretty=0)

#each of the terminal nodes are labeled Yes or No. The variables and the value of the splitting choice are shown at each terminal node. 

#Use model on test set, predict class labels 
tree.pred = predict(tree.carseats, carseats[-train,], type="class")

#Misclassification table to evaluate error 
with(carseats[-train,], table(tree.pred, High))

#Calculate error by summing up the diagonals and dividing by number of total predictions
mc = (71 + 42) / 150
mc
```

## Pruning using cross-validation
Pruning is a method to cut back the tree to prevent over-fitting. 

```{r Pruning}
#cross-validation to prune the tree using cv.tree
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)

#Sizes of the trees as they were pruned back, the deviances as the pruning proceeded, and cost complexity parameter used in the process.
cv.carseats

#Visualize 
plot(cv.carseats)

#Prune tree to a size of 12 
prune.carseats = prune.misclass(tree.carseats, best = 12)

#Visualize tree 
plot(prune.carseats)
text(prune.carseats, pretty=0)

#Evaluate on test set 
tree.pred = predict(prune.carseats, carseats[-train,], type="class")

#Misclassification 
with(carseats[-train,], table(tree.pred, High))

#Error 
mc_pruning = (66 + 41) / 150
mc_pruning

##pruning did not increase misclassification error by too much and resulted in a simpler tree!!
```
Pruning did not increase misclassification error by too much and resulted in a simpler tree!!

Decision trees suffer from high variance, meaning if you split the training data into 2 parts at random, and fit a decision tree to both halves, the results that you get could be very different.

Bagging and boosting are technique used to reduce the variance of your predictions.

## The Boston Housing Dataset 

506 Observations, 14 variables
Response Variable: medv (median value of owner-occupied homes for each suburb)

```{r The Boston Housing Dataset}
data("Boston")
boston = Boston
head(Boston)
```

## Bagging: Random Forest 

Bagging involves creating multiple copies of the original training dataset using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model. Each tree is built on a bootstrapped dataset, independent of the other trees.

Random Forest: Each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors.


```{r Random Forest}
#set seed for reproducibility 
set.seed(29)

#split into train and test sets (300 and 206 respectively)
train = sample(1:nrow(boston), 300)

#fit training subset of data to model 
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston

#summary of rf.boston gives information about the number of trees, the mean squared residuals (MSR), and the percentage of variance explained

#No. of variables tried at each split: 4 
#Each time the tree comes to split a node, 4 variables would be selected at random, then the split would be confined to 1 of those 4 variables.

##Lets try a range of mtry (number of variables selected at random at each split)

oob.err = double(13)
test.err = double(13)

#In a loop of mtry from 1 to 13, you first fit the randomForest to the train dataset
for(mtry in 1:13){
  fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
  oob.err[mtry] = fit$mse[350] ##extract Mean-squared-error 
  pred = predict(fit, boston[-train,]) #predict on test dataset
  test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 )) #compute test error
}

#Visualize 
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
```

## Boosting 

Boosting is another approach to improve the predictions resulting from a decision tree. Trees are grown sequentially: each tree is grown using information from previously grown trees. Each tree is fitted on a modified version of the original dataset.


```{r Boosting}
#Gradient Boosting Model
boost.boston = gbm(medv~., data = boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)

#Variable Importance Plot
summary(boost.boston)

#Visualize important variables of interest
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")

#Predict on test set
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = boston[-train,], n.trees = n.trees)
dim(predmat)

#Visualize Boosting Error Plot
boost.err = with(boston[-train,], apply( (predmat - medv)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")

```

## Homework

1. Attempt a regression tree-based method (not covered in this tutorial) on a reasonable dataset of your choice. Explain the results.

```{r}

# Regression Tree (Conditional Inference method)

# Conditional Inference Trees use a significance test procedure to select variables at each split. This overcomes a variable selection bias in the algorithms used in the traditional CART-based trees. Traditional approaches tend to select variables that have many possible splits or many missing values.

# set seed to make results reproducible 
set.seed(29)

# split data into train and test subset (300 and 206 respectively)
train = sample(1:nrow(boston), 300)

# MODEL

# fit train subset of data to model 
ctree.fit <- ctree(medv ~ ., 
                   data = boston,
                   subset = train,
                   control = ctree_control(maxdepth = Inf)) 
# 'maxdepth = Inf' means no restrictions are applied to tree size

# EVALUATION OF MODEL

# display the details of the tree
print(ctree.fit) 

# plot decision tree
plot(ctree.fit,
     main = "Regression CTree for Median Home Value") 

# Use model on test set, predict medv 
ctree.pred = predict(ctree.fit, boston[-train,])

# R-squared evaluation of predictions
cor(ctree.pred,boston[-train,]$medv)^2 

# The R-squared value of 0.76 is reasonable. The Conditional Inference approach ensures that the right-sized tree is grown without additional post-pruning or cross-validation. However, the depth of the tree is rather large, which makes interpretation more difficult than with less deep trees. In this case, pruning is undertaken for simplicity of plotting and interpretation, notwithstanding the primary role of pruning is to avoid overfitting the data.

# PRUNING

# Pruning can be easily done in this case by regrowing with a different control parameter.

# regrow the tree with smaller depth: maxdepth = 3
pruned.ctree.fit<- ctree(medv ~ ., 
                         data = boston,
                         subset = train,
                         control = ctree_control(maxdepth = 3))

# examine pruned tree
print(pruned.ctree.fit) 

# plot pruned tree
plot(pruned.ctree.fit,
     main = "Pruned Regression CTree for Median Home Value")

# Use pruned model on test set, predict medv 
pruned.ctree.pred = predict(pruned.ctree.fit, boston[-train,])

# R-squared evaluation of pruned model predictions
cor(pruned.ctree.pred,boston[-train,]$medv)^2 

# The R-squared value has dropped to 0.70. The pruned tree provides an easier set of rules, at the expense of a relatively small decrease in prediction accuracy. Importantly, it also should be more generalisable to other data. 

```


2. Attempt both a bagging and boosting method on a reasonable dataset of your choice. Explain the results.

```{r}

# BAGGING

data("Carseats")
carseats = Carseats

# convert quantitative variable Sales into a binary response 
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)

# set seed to make results reproducible 
set.seed(29)

# set up k-fold cross validation 
cvcontrol <- trainControl(method = "repeatedcv", 
                          number = 10,
                          allowParallel = TRUE)

# split data into train and test subset (250 and 150 respectively)
train = sample(1:nrow(carseats), 250)

# Fit train subset of data to model 
bagg.carseats = train(High~.-Sales, 
                      carseats, 
                      subset = train,
                      method = "treebag",
                      trControl = cvcontrol)

bagg.carseats 

# Use model on test set, predict class labels
bagg.pred = predict(bagg.carseats, 
                    carseats[-train,], 
                    type= "raw")

# Evaluation of bagging predictions

# confusion matrix
confusionMatrix(carseats[-train,]$High,bagg.pred) 
# accuracy 0.8

# ROC 

# 4 steps

# obtain predicted probabilites for test data
bagg.probs=predict(bagg.carseats,
                   carseats[-train,],
                   type = "prob")

# calculate ROC
ROC.bagg <- roc(carseats[-train,]$High,bagg.probs[,"Yes"])

# plot ROC
plot(ROC.bagg,col=c(6))

# calculate AOC 
auc(ROC.bagg) 
# 0.8823

# Evaluation shows reasonable model performance on test data, but room for improvement.

# BOOSTING

# set seed to make results reproducible 
set.seed(29)

# Fit train subset of data to model 
boost.carseats = train(High~.-Sales, 
                       carseats, 
                       subset = train,
                       method = "gbm", # gradient boosting
                       verbose = F,
                       trControl = cvcontrol)

boost.carseats

# Use model on test set, predict class labels
boost.pred = predict(boost.carseats, 
                     carseats[-train,], 
                     type = "raw")

# Evaluation of boosting predictions

# confusion matrix
confusionMatrix(carseats[-train,]$High,boost.pred) 
# accuracy 0.86

# ROC 

# obtain predicted probabilites for test data
boost.probs=predict(boost.carseats,
                    carseats[-train,],
                    type = "prob")

# calculate ROC
ROC.boost <- roc(carseats[-train,]$High,boost.probs[,"Yes"])

# plot ROC
plot(ROC.boost,col=c(3))

# calculate AOC 
auc(ROC.boost) 
# 0.9225

# Graphical comparison of bagging and boosting performances

plot(ROC.bagg,col=c(6)) # magenta-coloured curve
plot(ROC.boost, add = TRUE, col=c(3)) # green-coloured curve

# For this dataset, gradient boosting has outperformed bagging.

```


